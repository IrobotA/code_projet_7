# ğŸ“Š Comprehensive Data Quality Checker

A robust, automated data quality analysis system for SQL tables and DataFrames that checks **completeness**, **validity**, and **uniqueness** using pandas vectorized operations.

## ğŸ¯ Overview

This system addresses critical issues in your original data quality code by providing:

- âœ… **Clear separation** of NULL handling vs validity checking  
- âœ… **Vectorized pandas operations** for performance on large datasets
- âœ… **Consistent return types** across all validation functions
- âœ… **Comprehensive reporting** with detailed metrics
- âœ… **Auto-detection** of column data types
- âœ… **Database agnostic** - works with PostgreSQL, MySQL, SQL Server, SQLite

## ğŸš€ Quick Start

### Basic Usage with SQL Tables

```python
from data_quality_checker import DataQualityChecker

# Initialize with database connection
checker = DataQualityChecker("postgresql://user:pass@localhost:5432/db")

# Analyze any table
report = checker.analyze_table('customers', schema='public')

# Get human-readable summary
print(checker.generate_summary_report(report))
```

### Basic Usage with DataFrames

```python
from demo_quality_checker import DemoDataQualityChecker
import pandas as pd

# Initialize checker
checker = DemoDataQualityChecker()

# Analyze DataFrame
df = pd.read_csv('your_data.csv')
report = checker.analyze_dataframe(df, "your_table_name")

# Get summary report
print(checker.generate_summary_report(report))
```

## ğŸ“‹ Core Features

### ğŸ” **Completeness Analysis**
- Identifies missing values (NULL/NaN)
- Calculates completeness percentages
- Provides detailed missing value counts

### âœ… **Validity Checking**
- **VARCHAR/TEXT**: Length validation, pattern matching (email, phone, etc.)
- **Numeric**: Range validation, type checking
- **DateTime**: Date range validation, format checking
- **Boolean**: Valid value checking (True/False/1/0/etc.)

### ğŸ”‘ **Uniqueness Analysis**
- Duplicate detection and counting
- Uniqueness percentage calculation
- Top duplicate values identification

### ğŸ¯ **Auto Data Type Detection**
- Automatically detects pandas data types
- Applies appropriate validation rules
- Supports custom validation configurations

## ğŸ“Š What's Fixed from Your Original Code

### âŒ Original Problems:
1. **Logic Error**: `res and isna` created backwards logic
2. **Performance**: Manual loops instead of vectorized operations  
3. **Inconsistent Returns**: Mixed return types (Counter, list, nothing)
4. **Incomplete Functions**: Some validators didn't work

### âœ… Improvements:
1. **Clear Logic**: Separate NULL handling from validity checks
2. **Fast Operations**: Vectorized pandas operations throughout
3. **Consistent API**: All functions return structured dictionaries
4. **Complete Implementation**: All data types fully supported

## ğŸ›  Installation

```bash
pip install pandas numpy sqlalchemy
# Database-specific drivers (choose what you need):
pip install psycopg2      # PostgreSQL
pip install pymysql       # MySQL
pip install pyodbc        # SQL Server
```

## ğŸ“ Files Overview

- **`data_quality_checker.py`** - Main SQL database quality checker
- **`demo_quality_checker.py`** - Standalone demo with sample data
- **`example_usage.py`** - Comprehensive usage examples
- **`requirements.txt`** - Required dependencies

## ğŸ¯ Usage Examples

### 1. SQL Database Analysis

```python
from data_quality_checker import DataQualityChecker

# PostgreSQL
checker = DataQualityChecker("postgresql://user:pass@localhost:5432/db")
report = checker.analyze_table('customers')

# MySQL with sampling
checker = DataQualityChecker("mysql+pymysql://user:pass@localhost:3306/db")
report = checker.analyze_table('orders', sample_size=10000)

# SQL Server with custom rules
checker = DataQualityChecker("mssql+pyodbc://user:pass@server/db")
custom_rules = {
    'email_column': {'pattern': r'^[^@]+@[^@]+\.[^@]+$'},
    'age_column': {'min_value': 0, 'max_value': 120}
}
report = checker.analyze_table('users', custom_rules=custom_rules)
```

### 2. Quality Monitoring Workflow

```python
# Define quality thresholds
thresholds = {
    'completeness_minimum': 95.0,
    'validity_minimum': 90.0,
    'uniqueness_minimum': 99.0
}

# Monitor critical tables
for table in ['customers', 'orders', 'payments']:
    report = checker.analyze_table(table)
    
    # Check against thresholds and generate alerts
    for column, metrics in report['completeness'].items():
        if metrics['completeness_score'] < thresholds['completeness_minimum']:
            print(f"âš ï¸ {table}.{column}: Low completeness {metrics['completeness_score']}%")
```

### 3. Custom Validation Rules

```python
custom_rules = {
    'varchar': {
        'max_length_check': True,
        'pattern_checks': {
            'email': r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$',
            'phone': r'^\+?[\d\s\-\(\)]{10,}$'
        }
    },
    'numeric': {
        'range_check': True,
        'min_value': -1000000,
        'max_value': 1000000
    },
    'datetime': {
        'date_range_check': True,
        'min_date': '2000-01-01',
        'max_date': '2030-12-31'
    }
}

report = checker.analyze_table('your_table', custom_rules=custom_rules)
```

## ğŸ“Š Sample Output

```
ğŸ“Š DATA QUALITY REPORT
==================================================
Table: customers_demo
Rows: 1,070
Columns: 8

ğŸ” COMPLETENESS SUMMARY
âœ… customer_id: 93.7% complete
âœ… email: 95.3% complete
âš ï¸ age: 91.6% complete

âœ… VALIDITY SUMMARY
âœ… customer_id: 100.0% valid
âœ… email: 100.0% valid
âŒ registration_date: 98.0% valid
    âš ï¸  Dates before 1900-01-01

ğŸ”‘ UNIQUENESS SUMMARY
âš ï¸ customer_id: 93.5% unique
    ğŸ“Š 69 duplicates found
âŒ email: 0.8% unique
    ğŸ“Š 1,061 duplicates found
```

## ğŸ® Try the Demo

Run the interactive demo to see all features:

```bash
python3 demo_quality_checker.py
```

The demo creates sample data with intentional quality issues and shows:
- Before/after comparison with your original approach
- Comprehensive analysis of all data types
- Detailed reporting and insights

## ğŸ”§ Configuration Options

- **Sample Size**: Analyze subset of large tables
- **Custom Schemas**: Specify database schemas
- **Validation Rules**: Define custom validation logic
- **Quality Thresholds**: Set monitoring alerts
- **Export Formats**: JSON reports for further analysis

## ğŸš€ Performance Benefits

- **Vectorized Operations**: 10-100x faster than manual loops
- **Memory Efficient**: Handles large datasets efficiently
- **Scalable**: Works with millions of rows
- **Database Optimized**: Uses SQL sampling for large tables

## ğŸ”„ Integration Ready

This system is designed to be:
- **Automated**: Drop into ETL/data pipelines
- **Configurable**: Customize for your data standards
- **Monitorable**: Set up quality alerts and thresholds
- **Exportable**: Generate reports for stakeholders

## ğŸ¯ Next Steps

1. **Install dependencies**: `pip install -r requirements.txt`
2. **Run demo**: `python3 demo_quality_checker.py`
3. **Try examples**: Modify `example_usage.py` for your databases
4. **Integrate**: Add to your data pipelines
5. **Monitor**: Set up automated quality monitoring

Your data quality journey starts now! ğŸš€
